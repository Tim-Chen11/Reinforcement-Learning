# # https://blog.csdn.net/v_JULY_v/article/details/128965854?spm=1001.2014.3001.5506

# Got it ğŸ‘ Hereâ€™s the **menu / table of contents** of that CSDN blog article, but translated into **English** so you can follow more easily:

# ---

# ## ğŸ“‘ Article Structure (English)

# 1. **Preface**

#    * Why this article was written
#    * Features & purpose of writing

# 2. **Part I: RL Basics â€“ What is RL, MRP, and MDP**
#    1.1 Basic concepts needed to get started with reinforcement learning
#    Â Â Â 1.1.1 What is reinforcement learning: act according to policy â†’ perceive state â†’ receive reward
#    Â Â Â 1.1.2 Difference between RL and supervised learning + classification of RL methods
#    1.2 What is a Markov Decision Process (MDP) and the value function (Bellman equation)
#    Â Â Â 1.2.1 Prerequisites for MDP: state transition matrix, Markov process, Markov reward process (MRP)
#    Â Â Â 1.2.2 Markov Decision Process (MDP): states, actions, policy, value function + derivation of Bellman equation

# 3. **Part II: RL Advanced â€“ Three Table-Based Methods: DP, MC, TD**
#    2.1 Dynamic Programming (DP)
#    Â Â Â 2.1.1 What is dynamic programming
#    Â Â Â 2.1.2 Solving optimal policies with DP
#    2.2 Monte Carlo (MC) methods
#    2.3 Temporal-Difference (TD) learning and its differences from DP & MC

# 4. **Part III: Value-Based Learning â€“ From n-step Sarsa to Q-learning and DQN**
#    3.1 TD(0) control / Sarsa(0) and n-step Sarsa algorithms
#    3.2 Q-learning
#    3.3 Deep Q-Networks (DQN)

# 5. **Part IV: Policy-Based Learning â€“ From Policy Gradient, Actor-Critic to TRPO and PPO**
#    4.1 Policy Gradient and its main challenge: low sampling efficiency
#    Â Â Â 4.1.1 What is policy gradient + gradient computation/update process
#    Â Â Â 4.1.2 Avoiding one-time use of sampled data: importance sampling
#    (Later continues with baseline, advantage function, Actor-Critic, TRPO, PPO algorithms)

# ---